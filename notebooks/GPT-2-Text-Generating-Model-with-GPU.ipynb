{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2-Text-Generating-Model-with-GPU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "arhMf5Eq6T1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkuT25Kt7Rg2"
      },
      "source": [
        "# Train a GPT-2 Text-Generating Model \n",
        "\n",
        "<a href=\"https://github.com/DrSnowbird/gpt-2-simple-docker/notebooks/GPT-2-Text-Generating-Model-with-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook demonstrates how to run the [GPT-2 Text-Generating Model demo](https://github.com/DrSnowbird/gpt-2-simple-docker/). See the link for more details about the model, including evaluation metrics and credits.\n",
        "\n",
        "# Modifications by @DrSnowbird\n",
        "1. This notebook has been minorly modifed by @DrSnowbird to use **Tensorflow v2.x**\n",
        "2. Steps is reduced from **1,000 down to 200** since this Demo is just to show concept not for achieving good accuracy fine-tuned model.\n",
        "3. Removed the use of Google-Drive to simplify and avoid the problem in the gpt-2-simple copy-from-gdrive with hard-code path 'My Drive' which not existing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nRtnnXX7d6w"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdd3b0d-0630-4713-8997-1227e4f40d31"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1ac683-2b55-4fe8-b203-d5cd76653425"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  9 01:57:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`, e.g., `/models/124M`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa1dfec-49c1-44c8-efe5-06a61e4ac50a"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 261Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 8.21Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 808Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:12, 39.8Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 650Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 8.86Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 9.97Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc"
      },
      "source": [
        "# gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy6xByip2qoX",
        "outputId": "7a768140-5368-4887-e658-470c5e23516d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O \"shakespeare.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 01:57:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-12-09 01:57:23 (21.1 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"shakespeare.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "#gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81882763-436c-46ad-ff8c-3ddca6947550"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=200,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=200\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 338025 tokens\n",
            "Training...\n",
            "[10 | 60.28] loss=3.46 avg=3.46\n",
            "[20 | 115.55] loss=3.59 avg=3.53\n",
            "[30 | 170.91] loss=3.15 avg=3.40\n",
            "[40 | 226.18] loss=3.48 avg=3.42\n",
            "[50 | 281.36] loss=3.14 avg=3.36\n",
            "[60 | 336.56] loss=3.29 avg=3.35\n",
            "[70 | 391.79] loss=3.25 avg=3.34\n",
            "[80 | 446.97] loss=3.31 avg=3.33\n",
            "[90 | 502.19] loss=2.96 avg=3.29\n",
            "[100 | 557.45] loss=3.05 avg=3.27\n",
            "======== SAMPLE 1 ========\n",
            " I know I am in debt of it, and cannot bear it, and that I fear may prove to be grievous, or else, in that the issue of the money I issue from the people, I shall be found in debt.\n",
            "\n",
            "WARWICK:\n",
            "Now, sir, you shall pay, as you will; and all the rest shall be paid.\n",
            "\n",
            "Nurse:\n",
            "Sir, I have a son, Henry, named Richard, and shall be my wife.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I thank you, sir.\n",
            "\n",
            "WARWICK:\n",
            "It was well said by me, I have been born a Christian, and taught you for many years in the diocese,\n",
            "but to no great height.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I hope I shall never be more.\n",
            "\n",
            "Nurse:\n",
            "Sir, how is it, sir: here is a letter I have been sent to Paris, writing\n",
            "your approbation.\n",
            "\n",
            "WARWICK:\n",
            "How now!\n",
            "\n",
            "Nurse:\n",
            "Sir, I have made a very poor request; I pray you;\n",
            "you hear me, sir?\n",
            "\n",
            "WARWICK:\n",
            "Is this letter to you? What, pray, for all my letters?\n",
            "\n",
            "Nurse:\n",
            "Sir, if you may believe it, I have had the grace\n",
            "to go to Paris, where I have been to attend to the\n",
            "business of your office; and this letter, you say, may\n",
            "be of use to us all in Paris, if we may make a proper request\n",
            "to have it delivered. Do tell me how you come at this\n",
            "request, sir? For our purpose here I have,\n",
            "your grace,\n",
            "for which you have made great commendation.\n",
            "\n",
            "WARWICK:\n",
            "No, and my prayer is, sir, I hear it all very well.\n",
            "\n",
            "Nurse:\n",
            "Here are you, sir?\n",
            "\n",
            "WARWICK:\n",
            "Here's your father, my brother, Angelo, and my cousin,\n",
            "My father in this country, Angelo:\n",
            "Sir, how is my business?\n",
            "\n",
            "Nurse:\n",
            "Sir, I should be very much pleased to have you send you good words,\n",
            "which have set me in so great a place.\n",
            "\n",
            "WARWICK:\n",
            "So well did I tell you that your life should be so,\n",
            "that, in doing it, you have done a good end.\n",
            "\n",
            "Nurse:\n",
            "Sir, well I do think it well that you did,\n",
            "that you did not make the great request again for an hour.\n",
            "\n",
            "WARWICK:\n",
            "I'll warrant you that your business as a minister should not continue\n",
            "to this day: when, sir, at your request, I sent\n",
            "Your grace to Paris with some other business, which was very much\n",
            "parted in by the king.\n",
            "\n",
            "LADY CAPULEY:\n",
            "And so must this be.\n",
            "\n",
            "WARWICK:\n",
            "The prince had said nothing of your request nor was he\n",
            "a thing to be found: he, therefore, did say nothing.\n",
            "\n",
            "Nurse:\n",
            "Well, then, I have thought the man very well.\n",
            "\n",
            "WARWICK:\n",
            "Sir, the matter is your very own, in that, that I,\n",
            "any thing and I am all that you are here; and\n",
            "it hath been in your will as it is; and you, sir,\n",
            "I can warrant, that the king hath said nothing but,\n",
            "which he hath done,\n",
            "to speak a goodly and honourable resolution.\n",
            "\n",
            "LADY CAPULEY:\n",
            "My lord, a goodly resolution is well served; and\n",
            "I'll warrant you my grace to call it a noble thing and\n",
            "a goodly act.\n",
            "You shall be well satisfied.\n",
            "\n",
            "WARWICK:\n",
            "Sir, I shall find the case of your request very fit\n",
            "to the present proceedings. In this case I say,\n",
            "Sir, I have said no of the king or the queen.\n",
            "\n",
            "JULIET:\n",
            "Sir, since it is but time it must be done\n",
            "that you shall be in Paris next fall, I must come\n",
            "out: I believe the general will in the house,\n",
            "not to make your business, may well prove a noble deed.\n",
            "\n",
            "JOHN MARCLIFFE:\n",
            "The general will is my good, though I find fault\n",
            "With that. How is your business now?\n",
            "\n",
            "Nurse:\n",
            "What I shall inform you, you shall do it with due approbation\n",
            "Both at home and abroad. My business shall be\n",
            "to make the country as good as you have made it,\n",
            "with the state and good you have done.\n",
            "\n",
            "JOHN MARCLIFFE:\n",
            "There is no better man to have the good you have done\n",
            "than me.\n",
            "\n",
            "WARWICK:\n",
            "Well, well, well; but I would\n",
            "\n",
            "[110 | 635.99] loss=3.16 avg=3.26\n",
            "[120 | 691.23] loss=3.13 avg=3.24\n",
            "[130 | 746.46] loss=3.06 avg=3.23\n",
            "[140 | 801.70] loss=2.72 avg=3.19\n",
            "[150 | 856.94] loss=3.01 avg=3.18\n",
            "[160 | 912.19] loss=3.10 avg=3.17\n",
            "[170 | 967.41] loss=3.13 avg=3.17\n",
            "[180 | 1022.66] loss=2.65 avg=3.14\n",
            "[190 | 1077.87] loss=2.89 avg=3.12\n",
            "[200 | 1133.09] loss=2.75 avg=3.10\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "When you have got such a wife, be patient.\n",
            "But as for you, you must be patient.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I am sorry, sir, to be late.\n",
            "I must, as the time comes by: but now my son,\n",
            "I presume, shall soon show, and his tongue\n",
            "Is grown unwholesome.\n",
            "My noble lord, by this means you will answer my request.\n",
            "\n",
            "JOHN AUROLANO:\n",
            "Ay, good sir, I hear you.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Why, indeed, good madam.\n",
            "\n",
            "JOHN AUROLANO:\n",
            "I thank you for the answer.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "If you would, then I would ask your grace to do me harm.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I shall be ready for you when the queen comes,\n",
            "When the sun is up, but for such times as have but\n",
            "Lasted an hour in the vault, the sun shines\n",
            "With an open sky.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Now my grace, I have heard but two such things.\n",
            "Why, they were told much before you?\n",
            "\n",
            "JOHN AUROLANO:\n",
            "All this will I know now; but for now the answer I\n",
            "Will turn to my grace: for my lord is a\n",
            "Cleric, and the grace my husband deserves.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "He may well believe you; but he is an\n",
            "Assy, and the grace his son deserves.\n",
            "\n",
            "JOHN AUROLANO:\n",
            "It would be foolish to think otherwise,\n",
            "If it do not offend the love of God: but\n",
            "not, I think, to think so rashly:\n",
            "To think that God would punish me for wronging\n",
            "One night and then let me do Him a great harm!\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Then I would not make it but to lay waste\n",
            "My sanctuary: I beseech you, madam,\n",
            "Be your own master, your own master: we\n",
            "Have known the grace your wife will give,\n",
            "And I think she shall.\n",
            "\n",
            "JOHN AUROLANO:\n",
            "Now I must give her the gift that she needs.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I beseech you, in good time, to speak plainly;\n",
            "That your husband--his highness for heaven's sake--\n",
            "Is a very good man; and a most gentle man.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Prithee, good madam, speak not so.\n",
            "You are so full of doubt it was not worth hearing:\n",
            "But, in the hope of your grace, I dare you\n",
            "Say my husband is very good to my wife.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I say so: well, then, it was only for your wife's good,\n",
            "And for that good too, of your grace' sake.\n",
            "\n",
            "JOHN AUROLANO:\n",
            "Why, good madam, 'tis not for my wife's good,\n",
            "Which is, I thought, to be your good; for, madam,\n",
            "If it should be yours, that is for your comfort.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My love, 'tis your own; and I will not be\n",
            "More than my good husband's; but that you will not be\n",
            "With great sorrow. But what, fie, is this,\n",
            "That's as hard for your good as for mine?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Madam, you will need no aid.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Let me call upon you.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "We have some strange business in France,\n",
            "That I shall not answer with a fair answer,\n",
            "But with an indictment of certain\n",
            "Mistakes of certain kings. This is the gentleman\n",
            "Of Maricot, the most excellent son of\n",
            "Española; and he shall be my witness\n",
            "Before the royal house of Burgundy.\n",
            "\n",
            "VINCENTIO:\n",
            "I shall be your witness\n",
            "Upon me, and hear your voice from now on:\n",
            "But, madam, must you stay to answer us?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "You shall.\n",
            "\n",
            "VINCENTIO:\n",
            "Will you take your leave?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Yes, to stay a goodly night.\n",
            "\n",
            "VINCENTIO:\n",
            "O, it's good to have a night out\n",
            "And spend the night with sleep, and to cure a\n",
            "mold of sleepyness.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Now I doubt not; but since I may not, I'll\n",
            "go to bed, and sleep; till I be much\n",
            "\n",
            "\n",
            "[210 | 1210.30] loss=2.96 avg=3.10\n",
            "[220 | 1265.55] loss=2.63 avg=3.07\n",
            "[230 | 1320.81] loss=2.61 avg=3.05\n",
            "[240 | 1376.03] loss=2.69 avg=3.03\n",
            "[250 | 1431.27] loss=2.88 avg=3.03\n",
            "[260 | 1486.51] loss=3.17 avg=3.03\n",
            "[270 | 1541.75] loss=2.84 avg=3.02\n",
            "[280 | 1596.98] loss=2.86 avg=3.02\n",
            "[290 | 1652.20] loss=2.43 avg=2.99\n",
            "[300 | 1707.41] loss=2.65 avg=2.98\n",
            "Saving checkpoint/run1/model-300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "#gpt2.copy_checkpoint_to_gdrive(run_name='run1')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# This line is commented by our DrSnowbird!\n",
        "#\n",
        "# gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (**Runtime -> Restart Runtime**). Don't 'terminate' colab session - or, your will lost the fine-tuned trained model above!\n",
        "\n",
        "**Otherwise, the 'load_gpt2(sess, ...)' will fail**!\n",
        "\n",
        "**You will need to rerun imports as the following parts of code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEPQW3KBRtq0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7588b05b-580e-48fc-902e-7fc7e658c9d5"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-300\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7c630a-5f40-46d7-dc1e-694f5d4d9dbe"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With the new-made law,\n",
            "The divorcee is the widow;\n",
            "And the court's gaoler is the woman\n",
            "As the bride's court.\n",
            "What, in a bridal bed!\n",
            "\n",
            "KING HENRY VI:\n",
            "I do beseech you, think not I am gone.\n",
            "\n",
            "CLIFFORD:\n",
            "You say that I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, think not you.\n",
            "\n",
            "CLIFFORD:\n",
            "I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "I take my leave now.\n",
            "\n",
            "CLIFFORD:\n",
            "I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, then I am gone.\n",
            "\n",
            "Clown:\n",
            "O that only the couple should depart,\n",
            "And be gone.\n",
            "\n",
            "CLIFFORD:\n",
            "I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "And you the former.\n",
            "\n",
            "CLIFFORD:\n",
            "I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "But you the former.\n",
            "\n",
            "CLIFFORD:\n",
            "I am gone.\n",
            "\n",
            "KING HENRY VI:\n",
            "And you the former.\n",
            "\n",
            "Clown:\n",
            "O, that was a very pretty play.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, but a pretty one.\n",
            "\n",
            "Clown:\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, why, why, why, why, why, why, why, why, why, why, why?\n",
            "\n",
            "Clown:\n",
            "Why, why, why, why, why, why, why?\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, why, why, why, why, why?\n",
            "\n",
            "Clown:\n",
            "Why, why, why, why, why, why?\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, why, why, why, why, why?\n",
            "\n",
            "Clown:\n",
            "Why, why, why, why, why, why?\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, why, why, why, why?\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford,\n",
            "I have thought, good Clifford, to think that\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, why, why, why, why, why?\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford,\n",
            "I have thought, good Clifford, to think that\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "'Tis a tedious discourse,\n",
            "To be a tedious discourse.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "KING HENRY VI:\n",
            "O, I have thought, good Clifford, even\n",
            "We have not been to dinner.\n",
            "\n",
            "Clown:\n",
            "O, I have thought, good Clifford, even\n",
            "We have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saveListToFile(textList, outFileName):\n",
        "    with open(outFileName, 'w') as f:\n",
        "        f.write(textList)"
      ],
      "metadata": {
        "id": "xBO6wdK3GIFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = gpt2.generate(sess, return_as_list=True)[0]\n",
        "saveListToFile(text, 'generated-text-01.txt')"
      ],
      "metadata": {
        "id": "UCZE91HJEHZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2ea8c8-b67d-40f4-cb9a-38389575273f"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LORD FITZWATER:\n",
            "A stone is thrown to his head,\n",
            "That blasts forth a loud cry;\n",
            "That is my voice, if this be true.\n",
            "\n",
            "LORD FITZWATER:\n",
            "Your voices are no more.\n",
            "\n",
            "RICHARD:\n",
            "No more.\n",
            "\n",
            "ANGELO:\n",
            "What have you done?\n",
            "\n",
            "LADY CAPULET:\n",
            "I am hanged.\n",
            "\n",
            "ANGELO:\n",
            "What have you done?\n",
            "\n",
            "RICHARD:\n",
            "You know not;\n",
            "You know not.\n",
            "\n",
            "ANGELO:\n",
            "Nay, I do.\n",
            "\n",
            "LADY CAPULET:\n",
            "You know not,\n",
            "You know not, Richmond.\n",
            "\n",
            "ANGELO:\n",
            "Your voices are no more.\n",
            "\n",
            "RICHARD:\n",
            "No more.\n",
            "\n",
            "LADY CAPULET:\n",
            "No more.\n",
            "\n",
            "ANGELO:\n",
            "You have done it, you know not;\n",
            "You have not, you know not, you know not.\n",
            "\n",
            "LADY CAPULET:\n",
            "You have not done it, you know not;\n",
            "There is no more to it.\n",
            "\n",
            "ANGELO:\n",
            "No\n",
            "====================\n",
            "LORD:\n",
            "O, the heavens do not think the devil hath set us down\n",
            "So low,\n",
            "As we have set down,\n",
            "So low,\n",
            "So low,\n",
            "So low,\n",
            "To take an upright stand!\n",
            "\n",
            "CORIOLANUS:\n",
            "Well, sir, I'll undertake\n",
            "The exercise of my power in Rome.\n",
            "\n",
            "SICINIUS:\n",
            "That's my mind.\n",
            "\n",
            "CORIOLANUS:\n",
            "My mind!\n",
            "\n",
            "SICINIUS:\n",
            "I'll undertake it with my mind.\n",
            "\n",
            "CORIOLANUS:\n",
            "My mind!\n",
            "\n",
            "SICINIUS:\n",
            "I'll undertake it with my mind.\n",
            "\n",
            "CORIOLANUS:\n",
            "My mind!\n",
            "\n",
            "SICINIUS:\n",
            "What's your mind?\n",
            "\n",
            "CORIOLANUS:\n",
            "'Tis a mind to undertake a thought.\n",
            "\n",
            "SICINIUS:\n",
            "What's your mind?\n",
            "\n",
            "CORIOLANUS:\n",
            "A mind to undertake a thought.\n",
            "\n",
            "SICINIUS:\n",
            "I'll undertake it with my mind.\n",
            "\n",
            "CORIOLANUS:\n",
            "My mind!\n",
            "\n",
            "SIC\n",
            "====================\n",
            "LORD ROSS:\n",
            "O, thou art a daughter of a great prince,\n",
            "A jealous false queen, and, like a mad man,\n",
            "Like to a child, is as drowsy and drunk\n",
            "As a drunkard.\n",
            "\n",
            "ANGELO:\n",
            "Sir, I do tend to my letters\n",
            "Upon our study, and take your father's\n",
            "leave.\n",
            "\n",
            "CLIFFORD:\n",
            "I know you will not;\n",
            "I have a mind to stay, and so will your\n",
            "daughter, whose father is dead.\n",
            "\n",
            "ANGELO:\n",
            "Sir, I will go with you.\n",
            "\n",
            "CLIFFORD:\n",
            "Go thou to the Tower, and come I\n",
            "Of my father's time. I will not stay.\n",
            "\n",
            "ANGELO:\n",
            "Sir, I cannot lie with you:\n",
            "I have a mind to stay, and so will your\n",
            "daughter.\n",
            "\n",
            "CLIFFORD:\n",
            "I cannot lie with you:\n",
            "I have a mind to stay, and so will your\n",
            "daughter.\n",
            "\n",
            "ANGELO:\n",
            "Sir, I cannot lie with you,\n",
            "Because I have a mind to stay:\n",
            "I have a mind to stay, and so will your\n",
            "daughter.\n",
            "\n",
            "\n",
            "====================\n",
            "LORD WILLIAMSON:\n",
            "'Tis well, sir, you may not deny.\n",
            "\n",
            "KING RICHARD III:\n",
            "I have a warrant for your arrest,\n",
            "And, with my honest consent, you may\n",
            "Be put to death for your life, if you shall\n",
            "Come to my knowledge.\n",
            "\n",
            "KING RICHARD III:\n",
            "But die, my Lord, for truth! die!\n",
            "\n",
            "CLARENCE:\n",
            "No, I will not live to see him executed.\n",
            "\n",
            "KING RICHARD III:\n",
            "What is the warrant?\n",
            "\n",
            "CLARENCE:\n",
            "A warrant for the execution of his purpose.\n",
            "\n",
            "KING RICHARD III:\n",
            "And, if the truth be told, he hath a warrant\n",
            "For your death.\n",
            "\n",
            "CLARENCE:\n",
            "No warrant, Lord Lancaster, I am not able to give\n",
            "To my brother's brother, Clarence.\n",
            "\n",
            "KING RICHARD III:\n",
            "I will not, Lord Lancaster.\n",
            "\n",
            "CLARENCE:\n",
            "'Tis but\n",
            "That you, Lord Lancaster, have a warrant\n",
            "For your brother's death.\n",
            "\n",
            "KING RICHARD III:\n",
            "Nay, I would I were thus.\n",
            "\n",
            "CL\n",
            "====================\n",
            "LORD ROSS:\n",
            "Now, good masters,--\n",
            "The young prince, in his youth,\n",
            "Would not speak but as he would speak,\n",
            "And yet so would he--\n",
            "\n",
            "KING RICHARD III:\n",
            "Well, sir, I will--would you say so?\n",
            "\n",
            "SOMERSET:\n",
            "No, I will not.\n",
            "\n",
            "KING RICHARD III:\n",
            "Bastard, you would tell me, what was your name?\n",
            "\n",
            "SOMERSET:\n",
            "Sir, my name is Will, and I am the son of\n",
            "The king's brother, Henry.\n",
            "\n",
            "KING RICHARD III:\n",
            "Sirrah, my father's name is Clarence.\n",
            "\n",
            "SOMERSET:\n",
            "O, my good lord, my father's name is Richard.\n",
            "\n",
            "KING RICHARD III:\n",
            "Madam, I have known your father for twenty years\n",
            "And he is well known for his wit, his wit, his wit\n",
            "His wit, his wit, and his wit, and he is\n",
            "A gentleman of the king's most chosen son,\n",
            "Sir Richard Clarence.\n",
            "\n",
            "KING RICHARD III:\n",
            "What have you to say?\n",
            "\n",
            "SOMERSET:\n",
            "====================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac84a33-fe83-4425-a865-5bcf2a2fef61"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "print(f'gen_file=${gen_file}')\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen_file=$gpt2_gentext_20211209_030022.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "##########################################\n",
        "## Generate Text From The Pretrained Model\n",
        "## NOT using Fine-Tuned Model at all!\n",
        "##########################################\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_rjyzleVJNT"
      },
      "source": [
        "# Make sure that you 'restart the runtime and then jump here to start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD"
      },
      "source": [
        "USE_PRE_TRAINED_MODEL=False\n",
        "if USE_PRE_TRAINED_MODEL:\n",
        "  model_name = \"774M\"\n",
        "  gpt2.download_gpt2(model_name=model_name)\n",
        "  sess2 = gpt2.start_tf_sess()\n",
        "  gpt2.load_gpt2(sess2, model_name=model_name, reuse=False)\n",
        "  gpt2.generate(sess2,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}